{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import sys\n",
    "import umap\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data):\n",
    "    proportions = {-1: 85, 0: 10, 1: 5}\n",
    "    n_size = 20000\n",
    "\n",
    "    data_chunks = []\n",
    "\n",
    "    for label in [-1, 0, 1]:\n",
    "        this_label_rows_num = int(n_size / 100 * proportions[label])\n",
    "        data_chunks.append(data[data.label == label].sample(this_label_rows_num))\n",
    "\n",
    "    return pd.concat(data_chunks, axis=0)\n",
    "\n",
    "\n",
    "def preprocess_for_mapper(data):\n",
    "    NOT_USED_IN_MAPPER = [\"P_OFFLINEOPERATIONID\", \"P_DOCCATEGORY\", \"P_EKNPCODE\", \"P_CURRENCYCODE\", \n",
    "                          \"operation_id\", \"label\", \"fp_model_prediction\"]\n",
    "    \n",
    "    fp_model_prediction = data.fp_model_prediction\n",
    "    data_operation_ids = data.operation_id\n",
    "    data = data.drop(NOT_USED_IN_MAPPER, axis=1)\n",
    "    data = pd.DataFrame(data=scale(data), columns=data.columns)\n",
    "    \n",
    "    return data, fp_model_prediction, data_operation_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgenii/miniconda3/envs/py3_research/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/evgenii/tda/data_full.csv\")\n",
    "data_sample = sample_data(data)\n",
    "data_sample, fp_model_prediction, data_operation_ids = preprocess_for_mapper(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping on data shaped (20000, 9) using lens shaped (20000,)\n",
      "\n",
      "Creating 10 hypercubes.\n",
      "\n",
      "Created 8 edges and 22 nodes in 0:00:04.315893.\n",
      "Wrote visualization to: make_circles_keplermapper_output.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgenii/miniconda3/envs/py3_research/lib/python3.6/site-packages/kmapper/visuals.py:15: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  color_function = color_function.reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "import kmapper as km\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "res = mapper.map(fp_model_prediction, data_sample, DBSCAN(eps=1), nr_cubes=10)\n",
    "_ = mapper.visualize(res, path_html=\"make_circles_keplermapper_output.html\", color_function=1-fp_model_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_section_features():\n",
    "    path_to_shared_data = 'home/shared_files/'\n",
    "    path_to_baseamount_data = '/home/const.belev/spring_2018/beconstant/notebooks/datasets/baseamount_agg_mean_count_std_week/'\n",
    "    path_to_graph_data = '/home/const.belev/spring_2018/beconstant/notebooks/datasets/transaction_graph/'\n",
    "    path_to_agg_selection_models = '/home/const.belev/spring_2018/beconstant/notebooks/saved_models/agg_feature_selection/'\n",
    "\n",
    "    with open(os.path.join(path_to_graph_data, 'graph_trans_df.pkl'), 'rb') as handle:\n",
    "        graph_trans_df = pickle.load(handle)\n",
    "\n",
    "    with open(os.path.join(path_to_baseamount_data, 'colnames_source_in.pkl'), 'rb') as handle:\n",
    "        colnames_source_in = pickle.load(handle)\n",
    "\n",
    "    with open(os.path.join(path_to_baseamount_data, 'colnames_source_out.pkl'), 'rb') as handle:\n",
    "        colnames_source_out = pickle.load(handle)\n",
    "\n",
    "    with open(os.path.join(path_to_baseamount_data, 'colnames_target_in.pkl'), 'rb') as handle:\n",
    "        colnames_target_in = pickle.load(handle)\n",
    "\n",
    "    with open(os.path.join(path_to_baseamount_data, 'colnames_target_out.pkl'), 'rb') as handle:\n",
    "        colnames_target_out = pickle.load(handle)\n",
    "\n",
    "    colnames_all = np.array(colnames_source_in + colnames_source_out + colnames_target_in + colnames_target_out)\n",
    "    baseamount_agg_sparse = load_npz(os.path.join(path_to_baseamount_data, 'baseamount_agg_sparse.npz')).tocsr()\n",
    "    \n",
    "    return baseamount_agg_sparse, colnames_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (2,3,9,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 29.1 s, total: 2min 41s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "cleaned_off_ops = pd.read_csv(\"/home/const.belev/spring_2018/beconstant/notebooks/datasets/preprocessed/off_ops.csv\")\n",
    "section_features, section_colnames = load_section_features()\n",
    "\n",
    "with open(\"/home/evgenii/tda/operation_ids.pkl\", \"rb\") as f:\n",
    "    operation_ids = pd.Series(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFLINE_OPS_FEATURES_TO_TAKE = [\"P_CURRENCYCODE\", \"P_BASEAMOUNT\", \"P_CURRENCYAMOUNT\", \"P_EKNPCODE\", \"P_DOCCATEGORY\"]\n",
    "\n",
    "offline_operation_features = cleaned_off_ops[OFFLINE_OPS_FEATURES_TO_TAKE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_indices(indices):\n",
    "    this_op_ids = set(data_operation_ids.iloc[indices].values)\n",
    "    indices_mask = operation_ids.isin(this_op_ids)\n",
    "    \n",
    "    submatrix = section_features[np.where(indices_mask)[0]]\n",
    "    sparse_df = pd.SparseDataFrame(data=submatrix, columns=section_colnames).fillna(0)\n",
    "    offline_operation_features_for_idx = offline_operation_features.loc[indices_mask].fillna(0)\n",
    "    \n",
    "    for col in offline_operation_features_for_idx.columns:\n",
    "        sparse_df[col] = offline_operation_features_for_idx[col].values\n",
    "        \n",
    "    return sparse_df\n",
    "\n",
    "def get_features_for_cluster(cluster_name):\n",
    "    cluster_nodes = res[\"nodes\"][cluster_name]\n",
    "    \n",
    "    non_cluster_size = min(data_sample.shape[0]-len(cluster_nodes), 2 * len(cluster_nodes))\n",
    "    non_cluster_nodes = np.random.choice(list(set(range(data_sample.shape[0])) - set(cluster_nodes)), replace=False, size=non_cluster_size)\n",
    "\n",
    "    cluster_features = get_features_for_indices(cluster_nodes)\n",
    "    non_cluster_features = get_features_for_indices(non_cluster_nodes)\n",
    "    \n",
    "    return cluster_features, non_cluster_features\n",
    "\n",
    "cluster_features, non_cluster_features = get_features_for_cluster(\"cube9_cluster1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sum_columns(data):\n",
    "    for col in filter(lambda x: \"mean\" in x, data.columns):\n",
    "        cnt_col = col.replace(\"mean\", \"count\")\n",
    "        sum_col = col.replace(\"mean\", \"sum\")\n",
    "\n",
    "        data[sum_col] = pd.SparseSeries(data[col].to_dense() * data[cnt_col].to_dense())\n",
    "\n",
    "def remove_unnecessary_columns(data):\n",
    "    COLS_MASKS_TO_DROP = [\":std:\"]\n",
    "    cols_to_drop = list(filter(lambda x: any(mask in x for mask in COLS_MASKS_TO_DROP), data.columns))\n",
    "    data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "def process_columns(data):\n",
    "    #add_sum_columns(data)\n",
    "    remove_unnecessary_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import ks_2samp\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "def get_importances(cluster_features, non_cluster_features):\n",
    "    labels = np.hstack([np.ones_like(cluster_features.index), np.zeros_like(non_cluster_features.index)])\n",
    "    data = pd.concat([cluster_features, non_cluster_features], axis=0)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=250, n_jobs=-1)\n",
    "    rf.fit(data, labels)\n",
    "    \n",
    "    return sorted(zip(cluster_features.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def get_ks_scores(cluster_features, non_cluster_features):\n",
    "    ks_pvalues = []\n",
    "    ks_scores = []\n",
    "\n",
    "    for col in cluster_features.columns:\n",
    "        ks_res = ks_2samp(cluster_features[col].to_dense(), non_cluster_features[col].to_dense())\n",
    "        ks_pvalues.append(ks_res.pvalue)\n",
    "        ks_scores.append(ks_res.statistic)\n",
    "    fixed_pvalues = multipletests(ks_pvalues)\n",
    "    \n",
    "    return list(zip(cluster_features, ks_scores, fixed_pvalues[1], fixed_pvalues[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def get_feature_description(name, cluster_features, non_cluster_features):\n",
    "    bigger = cluster_features[name].mean() > non_cluster_features[name].mean()\n",
    "    relation_description = \"bigger\" if bigger else \"lesser\"\n",
    "    \n",
    "    tokens = name.split(\":\")\n",
    "    \n",
    "    if len(tokens) == 1:\n",
    "        return \"{name} is {relation} than usual\".format(name=name, relation=relation_description)\n",
    "    \n",
    "    type_description = \"Sum\" if tokens[1] == \"sum\" else \"Number\"\n",
    "    data_section_description = \"{0}={1}\".format(tokens[0], tokens[2])\n",
    "    if float(tokens[2]) == -1000:\n",
    "        data_section_description = \"without {0}\".format(tokens[0])\n",
    "    account_role_description = \"debit\" if tokens[3] == \"target\" else \"credit\"\n",
    "    \n",
    "    description = \"{type} of transactions with {section} in {role} account is {relation} that usual\".format(\n",
    "        type=type_description, section=data_section_description, role=account_role_description,\n",
    "        relation=relation_description)\n",
    "    \n",
    "    return description\n",
    "\n",
    "def _get_differences(cluster_features, non_cluster_features):\n",
    "    imps = get_importances(cluster_features, non_cluster_features)\n",
    "    #ks_res = get_ks_scores(cluster_features, non_cluster_features)\n",
    "    \n",
    "    samples = []\n",
    "    for name, imp in imps[:5]:\n",
    "        description = get_feature_description(name, cluster_features, non_cluster_features)\n",
    "        samples.append({\"description\": description, \"score\": imp * 100})\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "def explain_cluster(cluster_name, min_samples_to_explain=30):\n",
    "    cluster_features, non_cluster_features = get_features_for_cluster(cluster_name)\n",
    "    \n",
    "    if cluster_features.shape[0] < min_samples_to_explain:\n",
    "        return None\n",
    "    \n",
    "    process_columns(cluster_features)\n",
    "    process_columns(non_cluster_features)\n",
    "    \n",
    "    return _get_differences(cluster_features, non_cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display_tda(path_html):\n",
    "    iframe = '<iframe src=' + path_html \\\n",
    "            + ' width=100%% height=800 frameBorder=\"0\"></iframe>'\n",
    "    IPython.core.display.display(IPython.core.display.HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cube0_cluster0 15580\n",
      "cube0_cluster1 375\n",
      "cube0_cluster2 142\n",
      "cube0_cluster3 13\n",
      "cube1_cluster0 2768\n",
      "cube2_cluster0 12\n",
      "cube3_cluster0 6\n",
      "cube4_cluster0 14\n",
      "cube5_cluster0 42\n",
      "cube6_cluster0 42\n",
      "cube7_cluster0 69\n",
      "cube8_cluster0 241\n",
      "cube8_cluster1 6\n",
      "cube9_cluster0 600\n",
      "cube9_cluster1 66\n",
      "cube9_cluster2 20\n",
      "cube9_cluster3 5\n",
      "cube9_cluster4 25\n",
      "cube9_cluster5 5\n",
      "cube9_cluster8 5\n",
      "cube9_cluster7 5\n",
      "cube9_cluster6 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=make_circles_keplermapper_output.html width=100%% height=800 frameBorder=\"0\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters = res[\"nodes\"].items()\n",
    "\n",
    "for name, samples in clusters:\n",
    "    print(name, len(samples))\n",
    "    \n",
    "display_tda(\"make_circles_keplermapper_output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 9.66 s, total: 2min 42s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cluster_diffs = {name:explain_cluster(name) for name, _ in clusters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number of transactions with without P_EKNPCODE in credit account is bigger that usual</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of transactions with P_EKNPCODE=681 in credit account is bigger that usual</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_CURRENCYAMOUNT is bigger than usual</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of transactions with P_DOCCATEGORY=10 in credit account is bigger that usual</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of transactions with P_EKNPCODE=490 in credit account is bigger that usual</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             description  \\\n",
       "0  Number of transactions with without P_EKNPCODE in credit account is bigger that usual   \n",
       "1      Number of transactions with P_EKNPCODE=681 in credit account is bigger that usual   \n",
       "2                                                  P_CURRENCYAMOUNT is bigger than usual   \n",
       "3    Number of transactions with P_DOCCATEGORY=10 in credit account is bigger that usual   \n",
       "4      Number of transactions with P_EKNPCODE=490 in credit account is bigger that usual   \n",
       "\n",
       "   score  \n",
       "0   2.74  \n",
       "1   2.12  \n",
       "2   1.86  \n",
       "3   1.79  \n",
       "4   1.78  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 500, \"display.float_format\", \"{0:.2f}\".format):\n",
    "    display(cluster_diffs[\"cube9_cluster1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 Research",
   "language": "python",
   "name": "py3_research_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
