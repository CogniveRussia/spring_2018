{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import defaultdict, OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix, coo_matrix, csc_matrix\n",
    "\n",
    "### PUT YOUR PATH HERE (mine default is home/username/notebooks)\n",
    "path_to_data = '/home/shared_files/'\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', rc={'figure.figsize':(12,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, msg='operation', verbose=True):\n",
    "        self.msg = msg\n",
    "        self.verbose = verbose       \n",
    "    def __enter__(self, ):\n",
    "        self.start = time.clock()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.clock()\n",
    "        self.interval = self.end - self.start\n",
    "        if self.verbose:\n",
    "            print('{} took {:.3f}s'.format(self.msg, self.interval), flush=True)\n",
    "\n",
    "\n",
    "def df_categorical_variables_stat(df, max_cardinality=200):\n",
    "    for colname in df.columns:\n",
    "        print('next_column: {}'.format(colname))\n",
    "        uniq_vals = df[colname].unique()\n",
    "        nunique = len(uniq_vals)\n",
    "        print(\"# of uniqs: {}\".format(nunique))\n",
    "        if nunique < max_cardinality:\n",
    "            print('Viable Categorical. Value counts:')\n",
    "            print(df[colname].value_counts(dropna=False))\n",
    "        else:\n",
    "            print('High cardinality/Non categorical')\n",
    "        print((('-' * 80) + '\\n')*3)\n",
    "\n",
    "\n",
    "def process_client_indices(client_indices, trivial_to_nontrivial=None, default_null=-1000):\n",
    "    if trivial_to_nontrivial is None:\n",
    "        trivial_ids_to_nontrivial = defaultdict(lambda: -len(trivial_ids_to_nontrivial) - 2)\n",
    "\n",
    "    nontrivial_clients_ids = client_indices.copy()\n",
    "    nontrivial_clients_ids[nontrivial_clients_ids.isnull()] = default_null\n",
    "\n",
    "    nontrivial_clients_ids = nontrivial_clients_ids.map(str)\n",
    "\n",
    "    nontrivial_ids = nontrivial_clients_ids.map(lambda s: s.strip())\n",
    "\n",
    "    is_trivial = nontrivial_ids.map(lambda s: len(re.findall('^([\\d]+|-[\\d]+)', s)) == 0)\n",
    "    trivial_ids = nontrivial_ids[is_trivial]\n",
    "\n",
    "    for ti in trivial_ids:\n",
    "        trivial_ids_to_nontrivial[ti]\n",
    "    trivial_ids_to_nontrivial\n",
    "    nontrivial_ids[is_trivial] = trivial_ids.map(lambda s: str(trivial_ids_to_nontrivial[s]))\n",
    "\n",
    "    nontrivial_ids = nontrivial_ids.map(lambda s: re.findall('^([\\d]+|-[\\d]+)', s)[0]).map(int)\n",
    "    return nontrivial_ids, trivial_to_nontrivial\n",
    "\n",
    "\n",
    "def flatten_df_by_column(df, to_flatten, flatten_by, group_by, names_flatten_by=None):\n",
    "    columns_to_flatten = to_flatten\n",
    "    column_to_flatten_by = flatten_by\n",
    "    column_to_group_by = group_by\n",
    "    if names_flatten_by is None:\n",
    "        names_flatten_by = {\n",
    "            uniq_val: uniq_val\n",
    "            for uniq_val in df[column_to_flatten_by].unique()\n",
    "        }\n",
    "    if not isinstance(columns_to_flatten, (list, tuple, pd.Index)):\n",
    "        columns_to_flatten = [to_flatten]\n",
    "    frames_to_join = [\n",
    "        df[[column_to_group_by] + columns_to_flatten][df[column_to_flatten_by] == uniq_val]\n",
    "        for uniq_val in df[column_to_flatten_by].unique()\n",
    "    ]\n",
    "\n",
    "    for frame, uniq_val in zip(frames_to_join, df[flatten_by].unique()):\n",
    "        frame.set_index(column_to_group_by, inplace=True)\n",
    "        frame.columns = pd.MultiIndex.from_product([[uniq_val], frame.columns], names=[column_to_flatten_by, 'columns'])\n",
    "\n",
    "    res = frames_to_join[0].join(frames_to_join[1:], how='outer')\n",
    "    return res\n",
    "\n",
    "\n",
    "def join_ops_with_flatten_members(ops, flatten_ops_with_members, id_colname='ID', ops_columns_level_name=None):\n",
    "    if ops_columns_level_name is None:\n",
    "        ops_columns_level_name = str(list(flatten_ops_with_members.columns.levels[0]))\n",
    "    ops_to_join = ops.set_index(id_colname, drop=False)\n",
    "    ops_to_join.columns = pd.MultiIndex.from_product([[ops_columns_level_name], ops_to_join.columns],\n",
    "                                                     names=flatten_ops_with_members.columns.names)\n",
    "    joined_ops = ops_to_join.join(flatten_ops_with_members, how='left')\n",
    "    joined_ops.reset_index(drop=True, inplace=True)\n",
    "    return joined_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data csv and drop some columns and rows from off_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/const.belev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4,9,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading susp_ops took 4.106s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/const.belev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,21,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading susp_members took 4.383s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/const.belev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,8,9,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading off_ops took 50.854s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/const.belev/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,5,11,17,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading off_ops took 46.074s\n",
      "drop messy off_members columns took 3.027s\n",
      "drop off_members with OPERATIONID that are not in off_ops took 10.476s\n",
      "drop off_ops with OPERATIONID that are not in off_ops took 7.792s\n"
     ]
    }
   ],
   "source": [
    "with Timer('reading susp_ops', True):\n",
    "    susp_ops = pd.read_csv(os.path.join(path_to_data, 'susp_ops.csv'))\n",
    "with Timer('reading susp_members', True):\n",
    "    susp_members = pd.read_csv(os.path.join(path_to_data, 'susp_members.csv'))\n",
    "with Timer('reading off_ops', True):\n",
    "    off_ops = pd.read_csv(os.path.join(path_to_data, 'off_ops.csv'))\n",
    "with Timer('reading off_ops', True):\n",
    "    off_members = pd.read_csv(os.path.join(path_to_data, 'off_members.csv'))\n",
    "\n",
    "with Timer('drop messy off_members columns'):\n",
    "    off_members.drop(['P_DATE_INSERT', 'P_DATE_UPDATE', 'CHANGEDATE'], axis=1, inplace=True)\n",
    "\n",
    "with Timer('drop off_members with OPERATIONID that are not in off_ops'):\n",
    "    off_members = off_members[off_members.P_OPERATIONID.isin(off_ops.ID)].copy()\n",
    "    off_members.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with Timer('drop off_ops with OPERATIONID that are not in off_ops'):\n",
    "    off_ops = off_ops[off_ops.ID.isin(off_members.P_OPERATIONID)].copy()\n",
    "    off_ops.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean mess with P_CLIENTID and columns containing nans, drop stupid duplicates, Convert time to datetime format and sort operations by time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filling NaNs in off_members took 2.321s\n",
      "off_members client_ids processing took 43.630s\n",
      "dropping duplicate rows from off_members took 27.133s\n",
      "processing off_ops took 6.542s\n"
     ]
    }
   ],
   "source": [
    "fill_values_for_off_members = {\n",
    "    'P_BSCLIENTID': -1000,\n",
    "    'P_REGOPENDATE': '0000-00-00 00:00:00',\n",
    "    'P_BSACCOUNT': -1000,\n",
    "    'P_BANK': 'UNKNOWN',\n",
    "    'P_SDP': -1000,\n",
    "    'P_ORGFORM': -1000,\n",
    "    'P_BANKCITY': 'UNKNOWN'\n",
    "}\n",
    "\n",
    "with Timer('filling NaNs in off_members'):\n",
    "    off_members.fillna(value=fill_values_for_off_members, inplace=True)\n",
    "\n",
    "with Timer('off_members client_ids processing'):\n",
    "    off_members.loc[:, 'P_CLIENTID'], trivial_ids_to_nontrivial = process_client_indices(off_members.P_CLIENTID)\n",
    "\n",
    "with Timer('dropping duplicate rows from off_members'):\n",
    "    off_members.drop_duplicates(off_members.columns.drop(['ID', 'P_BSCLIENTID', 'P_BSACCOUNT']), inplace=True)\n",
    "    off_members.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with Timer('processing off_ops', True):\n",
    "    off_ops.loc[:, 'P_OPERATIONDATETIME'] = pd.to_datetime(off_ops['P_OPERATIONDATETIME'])\n",
    "    off_ops.sort_values(by='P_OPERATIONDATETIME', kind='mergesort', inplace=True)\n",
    "    off_ops.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map operation ID to 0-based one (assuming our ops already sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building counters for unique operation ID took 5.977s\n",
      "mapping original ID to 0-based for off_ops took 4.651s\n",
      "mapping original P_OPERATIONID to 0-based for off_members took 7.811s\n"
     ]
    }
   ],
   "source": [
    "with Timer('building counters for unique operation ID'):\n",
    "    operation_id_uniqs, operation_id_indices = np.unique(off_ops.ID.values, return_index=True)\n",
    "    operation_id_uniqs = operation_id_uniqs[operation_id_indices.argsort()]\n",
    "    operationid_counter = {u: i for i, u in enumerate(operation_id_uniqs)}\n",
    "    operationid_inv_counter = {v: k for k, v in operationid_counter.items()}\n",
    "\n",
    "del operation_id_uniqs\n",
    "del operation_id_indices\n",
    "\n",
    "with Timer('mapping original ID to 0-based for off_ops'):\n",
    "    off_ops.loc[:, 'ID'] = off_ops['ID'].map(operationid_counter.get)\n",
    "\n",
    "with Timer('mapping original P_OPERATIONID to 0-based for off_members'):\n",
    "    off_members.loc[:, 'P_OPERATIONID'] = off_members['P_OPERATIONID'].map(operationid_counter.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map P_CLIENTID to 0-based indices (to be able to work with sparse client matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort off-members by time-sorted operation ID took 6.929s\n",
      "build fast uniqs took 5.244s\n",
      "build clientid_counter took 0.404s\n",
      "mapping original P_CLIENTID to 0-based for off_members took 8.340s\n"
     ]
    }
   ],
   "source": [
    "with Timer('sort off-members by time-sorted operation ID'):\n",
    "    off_members.sort_values('P_OPERATIONID', kind='mergesort', inplace=True)\n",
    "\n",
    "clientids_sorted = off_members['P_CLIENTID'].values\n",
    "\n",
    "with Timer('build fast uniqs'):\n",
    "    clientids_uniqs, clientids_indices = np.unique(clientids_sorted[np.isfinite(clientids_sorted)], return_index=True)\n",
    "    clientids_uniqs = clientids_uniqs[clientids_indices.argsort()]\n",
    "\n",
    "with Timer('build clientid_counter'):\n",
    "    clientid_counter = {u: i for i, u in enumerate(clientids_uniqs)}\n",
    "    clientid_inv_counter = {v: k for k, v in clientid_counter.items()}\n",
    "\n",
    "with Timer('mapping original P_CLIENTID to 0-based for off_members'):\n",
    "    off_members.loc[:, 'P_CLIENTID'] = off_members['P_CLIENTID'].map(clientid_counter.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort off_members by time, compute seconds_from_start and seconds_from_last_client_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating seconds from start for off_ops took 1.490s\n",
      "joining off_members and seconds_from_start from off_ops took 35.163s\n",
      "stable sorting off_members by P_CLIENTID inplace took 22.342s\n",
      "retrieve deltas between client current and last operation took 107.782s\n",
      "sort back off_members by P_OPERATIONID took 14.859s\n"
     ]
    }
   ],
   "source": [
    "with Timer('calculating seconds from start for off_ops'):\n",
    "    off_ops['seconds_from_start'] = (off_ops.P_OPERATIONDATETIME - \\\n",
    "                                                off_ops.P_OPERATIONDATETIME.min()).dt.total_seconds()\n",
    "\n",
    "off_members.reset_index(drop=True, inplace=True)\n",
    "off_ops.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with Timer('joining off_members and seconds_from_start from off_ops'):\n",
    "    off_members = off_members.merge(off_ops[['ID', 'seconds_from_start']], how='inner', left_on='P_OPERATIONID', right_on='ID')\n",
    "    off_members.drop(['ID_y'], axis=1, inplace=True)\n",
    "    off_members.rename(columns={'ID_x': 'ID'}, inplace=True)\n",
    "\n",
    "with Timer('stable sorting off_members by P_CLIENTID inplace'):\n",
    "    off_members.sort_values('P_CLIENTID', kind='mergesort', inplace=True)\n",
    "\n",
    "with Timer('retrieve deltas between client current and last operation'):\n",
    "    off_members['seconds_from_last_client_op'] = off_members.groupby('P_CLIENTID')['seconds_from_start']\\\n",
    "                .agg('diff')\\\n",
    "                .sort_index(kind='mergesort')\\\n",
    "                .fillna(-100000)\n",
    "\n",
    "with Timer('sort back off_members by P_OPERATIONID'):\n",
    "    off_members.sort_values('P_OPERATIONID', kind='mergesort', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute acc persistence in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving member operationdatetime for each row from off_members took 103.967s\n",
      "computing acc_persistence for off_members took 29.033s\n"
     ]
    }
   ],
   "source": [
    "with Timer('retrieving member operationdatetime for each row from off_members'):\n",
    "    member_operationdatetime = off_ops.P_OPERATIONDATETIME[off_members.P_OPERATIONID]\n",
    "\n",
    "with Timer('computing acc_persistence for off_members'):\n",
    "    regopendate = pd.to_datetime(off_members['P_REGOPENDATE'],errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    acc_persistence = (member_operationdatetime.reset_index(drop=True) - regopendate.reset_index(drop=True)).dt.days.copy()\n",
    "    acc_persistence.loc[acc_persistence.isnull()] = -100000\n",
    "    off_members['acc_persistence'] = acc_persistence.values\n",
    "    off_members.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select columns from off_ops and mark cat/numeric ones\n",
    "### There also should be adding links info for past week for every op, but it was broken because problems with off_members, should be recalculated again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking selected columns from off_ops took 1.757s\n",
      "filling nans in selected_off_ops took 0.905s\n"
     ]
    }
   ],
   "source": [
    "selected_off_ops_columns = [\n",
    "    'ID',\n",
    "    'P_OPERATIONDATETIME',\n",
    "    'P_ISSUEDBID',\n",
    "    'P_BRANCH',\n",
    "    'P_CURRENCYCODE',\n",
    "    'P_EKNPCODE',\n",
    "    'P_DOCCATEGORY',\n",
    "    'P_TOEXTRACTBOOL',\n",
    "    'P_KFM_OPER_REASON',\n",
    "    'P_BS_OPER_TYPE',\n",
    "    'P_WAS_SEND',\n",
    "    'P_BASEAMOUNT',\n",
    "    'seconds_from_start',\n",
    "]\n",
    "\n",
    "selected_off_ops_ohe_columns = [\n",
    "    'P_ISSUEDBID',\n",
    "    'P_CURRENCYCODE',\n",
    "    'P_EKNPCODE',\n",
    "    'P_DOCCATEGORY',\n",
    "    'P_TOEXTRACTBOOL',\n",
    "    'P_KFM_OPER_REASON',\n",
    "    'P_BS_OPER_TYPE',\n",
    "    'P_WAS_SEND'\n",
    "]\n",
    "\n",
    "selected_off_ops_numeric_columns = [\n",
    "    'P_BASEAMOUNT'\n",
    "]\n",
    "\n",
    "\n",
    "with Timer('taking selected columns from off_ops'):\n",
    "    selected_off_ops = off_ops[selected_off_ops_columns].copy()\n",
    "\n",
    "off_ops_fillna_dict = {\n",
    "    'P_EKNPCODE': -1000,\n",
    "    'P_KFM_OPER_REASON': -1000\n",
    "}\n",
    "\n",
    "with Timer('filling nans in selected_off_ops'):\n",
    "    selected_off_ops.fillna(value=off_ops_fillna_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same column selection for off_members + build joined table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking selected columns from off_members took 3.272s\n",
      "building flatten selected members took 15.786s\n",
      "join offline operations with selected member columns took 6.691s\n"
     ]
    }
   ],
   "source": [
    "selected_off_members_columns = [\n",
    "    'P_CLIENTID',\n",
    "    'P_OPERATIONID',\n",
    "    'P_COUNTRYCODE',\n",
    "    'P_BANK',\n",
    "    'P_BANKCITY',\n",
    "    'P_SDP',\n",
    "    'P_BANK_CLIENT',\n",
    "    'P_CLIENT_TYPE',\n",
    "    'P_CLIENTROLE',\n",
    "    'P_BANKCOUNTRYCODE',\n",
    "    'P_BANKNAME',\n",
    "    'P_ORGFORM',\n",
    "    'seconds_from_last_client_op',\n",
    "    'acc_persistence'\n",
    "]\n",
    "\n",
    "selected_off_members_ohe_columns = [\n",
    "    'P_COUNTRYCODE',\n",
    "    'P_BANK',\n",
    "    'P_BANKCITY',\n",
    "    'P_SDP',\n",
    "    'P_BANK_CLIENT',\n",
    "    'P_CLIENT_TYPE',\n",
    "    'P_BANKCOUNTRYCODE',\n",
    "    'P_BANKNAME',\n",
    "    'P_ORGFORM',\n",
    "]\n",
    "\n",
    "selected_off_members_numeric_columns = [\n",
    "    'seconds_from_last_client_op',\n",
    "    'acc_persistence'\n",
    "]\n",
    "\n",
    "\n",
    "with Timer('taking selected columns from off_members'):\n",
    "    selected_off_members = off_members[selected_off_members_columns].copy()\n",
    "\n",
    "with Timer('building flatten selected members'):\n",
    "    selected_members_flatten = flatten_df_by_column(\n",
    "        selected_off_members,\n",
    "        to_flatten=list(selected_off_members.columns.drop(['P_OPERATIONID', 'P_CLIENTROLE'])),\n",
    "        flatten_by='P_CLIENTROLE',\n",
    "        group_by='P_OPERATIONID')\n",
    "\n",
    "with Timer('join offline operations with selected member columns'):\n",
    "    joined_off_ops = join_ops_with_flatten_members(selected_off_ops, selected_members_flatten, ops_columns_level_name='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build target variable and set joined_off_ops.target with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401500/401500 [00:16<00:00, 24445.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting proper labels for learning took 16.497s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting susp target for every operation took 4.238s\n"
     ]
    }
   ],
   "source": [
    "susp_ops = susp_ops[~susp_ops.P_OFFLINEOPERATIONID.isnull()].copy()\n",
    "susp_ops.loc[susp_ops.P_OPERATION_LIST.isnull(), 'P_OPERATION_LIST'] = ''\n",
    "error_count = 0\n",
    "\n",
    "with Timer('setting proper labels for learning'):\n",
    "    target_mapped = defaultdict(lambda: -1)\n",
    "\n",
    "    for i, row in tqdm(susp_ops[['P_OFFLINEOPERATIONID', 'P_SENDTOKFMBOOL']].iterrows(), total=len(susp_ops)):\n",
    "        try:\n",
    "            off_op_id, to_kfm = operationid_counter[int(row.P_OFFLINEOPERATIONID)], row.P_SENDTOKFMBOOL\n",
    "        except KeyError:\n",
    "            # lol we found OPERATIONID that is not in off_ops anymore\n",
    "            error_count += 1\n",
    "            pass\n",
    "        if off_op_id not in target_mapped:\n",
    "            target_mapped[off_op_id] = 0 if to_kfm == 2 else 1\n",
    "        else:\n",
    "            cur_target = target_mapped[off_op_id]\n",
    "            if cur_target == 0 and to_kfm != 2:\n",
    "                target_mapped[off_op_id] = 1\n",
    "\n",
    "with Timer('setting susp target for every operation'):\n",
    "    joined_off_ops['all', 'target'] = joined_off_ops['all', 'ID'].map(target_mapped.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex(levels=[[1, 2, 5, 'all'], ['ID', 'P_BANK', 'P_BANKCITY', 'P_BANKCOUNTRYCODE', 'P_BANKNAME', 'P_BANK_CLIENT', 'P_BASEAMOUNT', 'P_BRANCH', 'P_BS_OPER_TYPE', 'P_CLIENTID', 'P_CLIENT_TYPE', 'P_COUNTRYCODE', 'P_CURRENCYCODE', 'P_DOCCATEGORY', 'P_EKNPCODE', 'P_ISSUEDBID', 'P_KFM_OPER_REASON', 'P_OPERATIONDATETIME', 'P_ORGFORM', 'P_SDP', 'P_TOEXTRACTBOOL', 'P_WAS_SEND', 'acc_persistence', 'seconds_from_last_client_op', 'seconds_from_start', 'target']],\n",
       "           labels=[[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3], [0, 17, 15, 7, 12, 14, 13, 20, 16, 8, 21, 6, 24, 9, 11, 1, 2, 19, 5, 10, 3, 4, 18, 23, 22, 9, 11, 1, 2, 19, 5, 10, 3, 4, 18, 23, 22, 9, 11, 1, 2, 19, 5, 10, 3, 4, 18, 23, 22, 25]],\n",
       "           names=['P_CLIENTROLE', 'columns'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_off_ops.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P_CLIENTROLE  columns                    \n",
       "all           ID                             0.000000\n",
       "              P_OPERATIONDATETIME            0.000000\n",
       "              P_ISSUEDBID                    0.000000\n",
       "              P_BRANCH                       0.000000\n",
       "              P_CURRENCYCODE                 0.000000\n",
       "              P_EKNPCODE                     0.000000\n",
       "              P_DOCCATEGORY                  0.000000\n",
       "              P_TOEXTRACTBOOL                0.000000\n",
       "              P_KFM_OPER_REASON              0.000000\n",
       "              P_BS_OPER_TYPE                 0.000000\n",
       "              P_WAS_SEND                     0.000000\n",
       "              P_BASEAMOUNT                   0.000000\n",
       "              seconds_from_start             0.000000\n",
       "1             P_CLIENTID                     0.132619\n",
       "              P_COUNTRYCODE                  0.132619\n",
       "              P_BANK                         0.132619\n",
       "              P_BANKCITY                     0.132619\n",
       "              P_SDP                          0.132619\n",
       "              P_BANK_CLIENT                  0.132619\n",
       "              P_CLIENT_TYPE                  0.132619\n",
       "              P_BANKCOUNTRYCODE              0.132619\n",
       "              P_BANKNAME                     0.132619\n",
       "              P_ORGFORM                      0.132619\n",
       "              seconds_from_last_client_op    0.132619\n",
       "              acc_persistence                0.132619\n",
       "2             P_CLIENTID                     0.468761\n",
       "              P_COUNTRYCODE                  0.468761\n",
       "              P_BANK                         0.468761\n",
       "              P_BANKCITY                     0.468761\n",
       "              P_SDP                          0.468761\n",
       "              P_BANK_CLIENT                  0.468761\n",
       "              P_CLIENT_TYPE                  0.468761\n",
       "              P_BANKCOUNTRYCODE              0.468761\n",
       "              P_BANKNAME                     0.468761\n",
       "              P_ORGFORM                      0.468761\n",
       "              seconds_from_last_client_op    0.468761\n",
       "              acc_persistence                0.468761\n",
       "5             P_CLIENTID                     0.997265\n",
       "              P_COUNTRYCODE                  0.997265\n",
       "              P_BANK                         0.997265\n",
       "              P_BANKCITY                     0.997265\n",
       "              P_SDP                          0.997265\n",
       "              P_BANK_CLIENT                  0.997265\n",
       "              P_CLIENT_TYPE                  0.997265\n",
       "              P_BANKCOUNTRYCODE              0.997265\n",
       "              P_BANKNAME                     0.997265\n",
       "              P_ORGFORM                      0.997265\n",
       "              seconds_from_last_client_op    0.997265\n",
       "              acc_persistence                0.997265\n",
       "all           target                         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_off_ops.isnull().mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_multi_index = pd.MultiIndex(levels=[[1, 2, 5, 'all'], ['ID', 'P_BASEAMOUNT', 'P_CLIENTID', 'P_EKNPCODE', 'P_OPERATIONDATETIME', 'seconds_from_start', 'target']],\n",
    "           labels=[[3, 3, 0, 1, 2, 3, 3, 3, 3], [0, 1, 2, 2, 2, 3, 4, 5, 6]],\n",
    "           names=['P_CLIENTROLE', 'columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_trans_df = joined_off_ops[graph_multi_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('graph_trans_df.pkl', 'wb') as handle:\n",
    "    pickle.dump(graph_trans_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_index = pd.Index(data=[\n",
    "    'ID',\n",
    "    'P_BASEAMOUNT',\n",
    "    'id_sender',\n",
    "    'id_receiver',\n",
    "    'id_benef',\n",
    "    'P_EKNPCODE',\n",
    "    'P_OPERATIONDATETIME',\n",
    "    'seconds_from_start',\n",
    "    'target'], labels=list(range(9)), names=['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_trans_df.columns = graph_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('writing graph_trans_df to disk'):\n",
    "    graph_trans_df.to_csv('graph_trans_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_level_columns(df, columns, copy=False):\n",
    "    if copy:\n",
    "        return df.loc[:, df.columns.map(lambda x: x[1] in columns)].copy()\n",
    "    else:\n",
    "        return df.loc[:, df.columns.map(lambda x: x[1] in columns)]\n",
    "\n",
    "joined_off_ops_cat = get_second_level_columns(joined_off_ops, \n",
    "                                              set(selected_off_ops_ohe_columns).union(set(selected_off_members_ohe_columns)))\n",
    "\n",
    "joined_off_ops_num = get_second_level_columns(joined_off_ops,\n",
    "                                              set(selected_off_ops_numeric_columns).union(set(selected_off_members_numeric_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only operations with existing clients with roles 1 and 2 (transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_direct_trans = joined_off_ops.loc[(~joined_off_ops[1, 'P_CLIENTID'].isnull() \n",
    "                                          & ~joined_off_ops[2, 'P_CLIENTID'].isnull()), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with clientrole 5 (beneficial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_direct_transfers = joined_direct_trans.drop(5, axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_direct_transfers_cat = get_second_level_columns(joined_direct_transfers, \n",
    "                                              set(selected_off_ops_ohe_columns).union(set(selected_off_members_ohe_columns)))\n",
    "\n",
    "joined_direct_transfers_num = get_second_level_columns(joined_direct_transfers,\n",
    "                                              set(selected_off_ops_numeric_columns).union(set(selected_off_members_numeric_columns)))\n",
    "\n",
    "joined_direct_transfers_target = joined_direct_transfers['all', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex(levels=[[1, 2, 5, 'all'], ['ID', 'P_BANK', 'P_BANKCITY', 'P_BANKCOUNTRYCODE', 'P_BANKNAME', 'P_BANK_CLIENT', 'P_BASEAMOUNT', 'P_BRANCH', 'P_BS_OPER_TYPE', 'P_CLIENTID', 'P_CLIENT_TYPE', 'P_COUNTRYCODE', 'P_CURRENCYCODE', 'P_DOCCATEGORY', 'P_EKNPCODE', 'P_ISSUEDBID', 'P_KFM_OPER_REASON', 'P_OPERATIONDATETIME', 'P_ORGFORM', 'P_SDP', 'P_TOEXTRACTBOOL', 'P_WAS_SEND', 'acc_persistence', 'seconds_from_last_client_op', 'seconds_from_start', 'target']],\n",
      "           labels=[[3, 0, 0, 1, 1], [6, 23, 22, 23, 22]],\n",
      "           names=['P_CLIENTROLE', 'columns'])\n"
     ]
    }
   ],
   "source": [
    "print(joined_direct_transfers_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P_CLIENTROLE  columns                    \n",
       "all           P_BASEAMOUNT                   0.0\n",
       "1             seconds_from_last_client_op    0.0\n",
       "              acc_persistence                0.0\n",
       "2             seconds_from_last_client_op    0.0\n",
       "              acc_persistence                0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_direct_transfers_num.isnull().mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151075.01, 136206.83,  59613.06, ...,  10053.23, 265901.94,\n",
       "        29665.83])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_direct_transfers_num.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we use n_components = sqrt(len(one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P_CLIENTROLE  columns          \n",
       "all           P_ISSUEDBID             5\n",
       "              P_CURRENCYCODE         29\n",
       "              P_EKNPCODE            287\n",
       "              P_DOCCATEGORY          13\n",
       "              P_TOEXTRACTBOOL         2\n",
       "              P_KFM_OPER_REASON      11\n",
       "              P_BS_OPER_TYPE        164\n",
       "              P_WAS_SEND              3\n",
       "1             P_COUNTRYCODE          64\n",
       "              P_BANK                564\n",
       "              P_BANKCITY             88\n",
       "              P_SDP                   2\n",
       "              P_BANK_CLIENT           2\n",
       "              P_CLIENT_TYPE           3\n",
       "              P_BANKCOUNTRYCODE      73\n",
       "              P_BANKNAME            536\n",
       "              P_ORGFORM              15\n",
       "2             P_COUNTRYCODE         101\n",
       "              P_BANK               1357\n",
       "              P_BANKCITY            250\n",
       "              P_SDP                   2\n",
       "              P_BANK_CLIENT           2\n",
       "              P_CLIENT_TYPE           3\n",
       "              P_BANKCOUNTRYCODE      86\n",
       "              P_BANKNAME           1163\n",
       "              P_ORGFORM              15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_direct_transfers_cat.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('building one-hot representation for every categorical column'):\n",
    "    joined_direct_transfers_ohe_dict = {\n",
    "        (clientrole, colname): pd.get_dummies(joined_direct_transfers_cat[clientrole][[colname]],\n",
    "                                columns=[colname],\n",
    "                                prefix=[colname])\n",
    "        for clientrole, colname in joined_direct_transfers_cat.columns\n",
    "    }\n",
    "\n",
    "with Timer('PCA initializating for every column'):\n",
    "    pca_ohe = {\n",
    "        (clientrole, colname): PCA(n_components=max(1, int(np.log1p(len(ohe_df.columns)))))\n",
    "        for (clientrole, colname), ohe_df in joined_direct_transfers_ohe_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate = 0.7\n",
    "train_size = int(train_rate * len(joined_direct_transfers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [19:45<00:00, 45.59s/it]\n"
     ]
    }
   ],
   "source": [
    "transformed_train_ohe = {}\n",
    "transformed_test_ohe = {}\n",
    "\n",
    "for role_colname in tqdm(joined_direct_transfers_cat.columns):\n",
    "    vals = joined_direct_transfers_ohe_dict[role_colname].values.astype(np.float32)\n",
    "    transformed_train_ohe[role_colname] = pca_ohe[role_colname].fit_transform(vals[:train_size])\n",
    "    transformed_test_ohe[role_colname] = pca_ohe[role_colname].transform(vals[train_size:])\n",
    "    del vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe_pca = np.concatenate([transformed_train_ohe[role_colname] for role_colname in joined_direct_transfers_cat.columns], axis=1)\n",
    "test_ohe_pca = np.concatenate([transformed_test_ohe[role_colname] for role_colname in joined_direct_transfers_cat.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non_ohe_role_columns = (list([('all', colname) for colname in selected_off_ops_numeric_columns])\n",
    "#                        + list(pd.MultiIndex.from_product([[1, 2, 5], selected_off_members_numeric_columns])))\n",
    "vals_non_ohe = joined_direct_transfers_num.values.astype(np.float32)\n",
    "train_non_ohe = vals_non_ohe[:train_size]\n",
    "test_non_ohe = vals_non_ohe[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest = np.concatenate([train_ohe_pca, train_non_ohe], axis=1), np.concatenate([test_ohe_pca, test_non_ohe], axis=1)\n",
    "ytrain, ytest = joined_direct_transfers_target.values[:train_size], joined_direct_transfers_target.values[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "isof = IsolationForest(n_estimators=1000, contamination = (ytrain == 1).mean(), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e8d937b44348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0misof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/iforest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         self.threshold_ = -sp.stats.scoreatpercentile(\n\u001b[0;32m--> 204\u001b[0;31m             -self.decision_function(X), 100. * (1. - self.contamination))\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/iforest.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mdepths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mdepths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_average_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0m_average_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/iforest.py\u001b[0m in \u001b[0;36m_average_path_length\u001b[0;34m(n_samples_leaf)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0maverage_path_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         average_path_length[not_mask] = 2. * (\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples_leaf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meuler_gamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                 n_samples_leaf[not_mask] - 1.) / n_samples_leaf[not_mask]\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "isof.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.concatenate([xtrain, xtest], 0), np.concatenate([ytrain, ytest], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('xtrain.npy', xtrain)\n",
    "#np.save('xtest.npy', xtest)\n",
    "#np.save('ytrain.npy', ytrain)\n",
    "#np.save('ytest.npy', ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kek = np.load('xtest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7487276525538096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(off_members.P_REGOPENDATE, errors='coerce', format='%Y-%m-%d %H:%M:%S').isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_datetime = off_ops.P_OPERATIONDATETIME[off_members.P_OPERATIONID.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_members.'P_REGOPENDATE', lambda:  pd.to_datetime(off_members['P_REGOPENDATE'],\n",
    "                                               errors='coerce',\n",
    "                                               format='%Y-%m-%d %H:%M:%S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_unfreq(series, min_freq=100, filler='UNFREQ'):\n",
    "    series = series.copy()\n",
    "    counts = series.value_counts()\n",
    "    series[series.isin(counts[counts < min_freq]).index] = filler\n",
    "    return series\n",
    "\n",
    "useful_off_members_solumns = [\n",
    "    'P_REGOPENDATE',\n",
    "    'P_COUNTRYCODE',\n",
    "    'P_BANKCITY',\n",
    "    'P_SDP',\n",
    "    'P_BANK_CLIENT',\n",
    "    'P_CLIENT_TYPE',\n",
    "    'P_CLIENTROLE',\n",
    "    'P_BANKCOUNTRYCODE',\n",
    "    'P_BANKNAME',\n",
    "    'P_ORGFORM',\n",
    "    'seconds_from_start',\n",
    "    'seconds_from_last_client_op'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "selected_off_members_columns_processing_pipeline = OrderedDict([\n",
    "    ('P_REGOPENDATE', lambda:  pd.to_datetime(off_members['P_REGOPENDATE'], errors='coerce', format='%Y-%m-%d %H:%M:%S')),\n",
    "    ('P_COUNTRYCODE', lambda: fill_unfreq(off_members['P_COUNTRYCODE'])),\n",
    "    ('P_BANKCITY', lambda: fill_unfreq(off_members['P_BANKCITY'])),\n",
    "    ('P_SDP', lambda: off_members['P_SDP'].copy()),\n",
    "    ('P_BANK_CLIENT', lambda: off_members['P_BANK_CLIENT'].copy()),\n",
    "    ('P_CLIENT_TYPE', lambda: off_members['P_CLIENT_TYPE'].copy()),\n",
    "    ('P_CLIENTROLE', lambda: off_members['P_CLIENTROLE'].copy()),\n",
    "    ('P_BANKCOUNTRYCODE', lambda: fill_unfreq(off_members['P_BANKCOUNTRYCODE'], min_freq=75)),\n",
    "    ('P_BANKNAME', lambda: fill_unfreq(off_members['P_BANKNAME'], min_freq=75)),\n",
    "    ('P_ORGFORM', lambda: off_members['P_ORGFORM'].copy()),\n",
    "    ('seconds_from_start', lambda: off_members['seconds_from_start'].copy()),\n",
    "    ('seconds_from_last_client_op', lambda: off_members['seconds_from_last_client_op'].copy()),\n",
    "])\n",
    "\n",
    "\n",
    "simple_off_members_columns_pipeline = OrderedDict(\n",
    "    [('P_REGOPENDATE', lambda:  pd.to_datetime(off_members['P_REGOPENDATE'],\n",
    "                                               errors='coerce',\n",
    "                                               format='%Y-%m-%d %H:%M:%S'))]\n",
    "    + [colname, lambda: off_members[colname].copy() for colname in useful_off_members_solumns[1:]])\n",
    "\n",
    "\n",
    "with Timer('process selected fields from off_members'):\n",
    "    # There are some weird dates in P_REGOPENDATE, raising errors will force them to be  NaT\n",
    "    p_regopendate = pd.to_datetime(off_members.P_REGOPENDATE, errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Rare countrycodes would be set as 'UNFREQ', NaNs will be 'UNKNOWN'\n",
    "    p_countrycode = off_members.P_COUNTRYCODE.copy()\n",
    "    countrycode_counts = p_countrycode.value_counts()\n",
    "    p_countrycode[p_countrycode.isin(countrycode_counts[countrycode_counts <= 100].index)] = 'UNFREQ'\n",
    "    p_countrycode[p_countrycode.isnull()] = 'UNKNOWN'\n",
    "\n",
    "    # Same for bankcity\n",
    "    p_bankcity = off_members.P_BANKCITY.copy()\n",
    "    bankcity_counts = p_bankcity.value_counts()\n",
    "    p_bankcity[p_bankcity.isin(bankcity_counts[bankcity_counts <= 100].index)] = 'UNFREQ'\n",
    "    p_bankcity[p_bankcity.isnull()] = 'UNKNOWN'\n",
    "\n",
    "    # for bank there is only a few NaN values, so it will be 'UNFREQ' aswell\n",
    "    p_bank = off_members.P_BANK.copy()\n",
    "    bank_counts = p_bank.value_counts()\n",
    "    p_bank[p_bank.isin(bank_counts[bank_counts <= 100].index)] = 'UNFREQ'\n",
    "    p_bank[p_bank.isnull()] = 'UNFREQ'\n",
    "\n",
    "    # SDP nas only NaN/13.0 values, so simply encode NaN with -1k\n",
    "    p_sdp = off_members.P_SDP.copy()\n",
    "    p_sdp[p_sdp.isnull()] = -1000\n",
    "\n",
    "    # No missing values\n",
    "    p_bank_client = off_members.P_BANK_CLIENT.copy()\n",
    "\n",
    "    # No missing values\n",
    "    p_client_type = off_members.P_CLIENT_TYPE.copy()\n",
    "\n",
    "    # No missing values\n",
    "    p_clientrole = off_members.P_CLIENTROLE.copy()\n",
    "    p_seconds_from_start = off_members.seconds_from_start.copy()\n",
    "    p_seconds_from_last_client_op = off_members.seconds_from_last_client_op.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                             0.0\n",
       "P_OPERATIONID                  0.0\n",
       "P_CLIENTID                     0.0\n",
       "P_BSCLIENTID                   0.0\n",
       "P_BANK_CLIENT                  0.0\n",
       "P_REGOPENDATE                  0.0\n",
       "P_COUNTRYCODE                  0.0\n",
       "P_CLIENT_TYPE                  0.0\n",
       "P_CLIENTROLE                   0.0\n",
       "P_CLIENTKIND                   0.0\n",
       "P_ACCOUNT                      0.0\n",
       "P_BSACCOUNT                    0.0\n",
       "P_BANK                         0.0\n",
       "P_BANKCOUNTRYCODE              0.0\n",
       "P_BANKNAME                     0.0\n",
       "P_IPDL                         0.0\n",
       "P_USERNAME                     0.0\n",
       "P_SDP                          0.0\n",
       "P_ORGFORM                      0.0\n",
       "P_BANKCITY                     0.0\n",
       "P_OPER_DATE                    0.0\n",
       "P_OPERATIONDATE                0.0\n",
       "seconds_from_start             0.0\n",
       "seconds_from_last_client_op    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_members.isnull().mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
