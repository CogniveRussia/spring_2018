{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import IPython\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import kmapper as km\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import t_sne, isomap\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "import cx_Oracle\n",
    "\n",
    "# oracle\n",
    "oracle_user = \"ro_user\"\n",
    "oracle_pass = \"ro_user\"\n",
    "oracle_scheme = \"aml_evraz\"\n",
    "oracle_host = \"192.168.101.13/rnd\"\n",
    "oracle_connection = cx_Oracle.connect(oracle_user, oracle_pass, oracle_host, encoding = \"UTF-8\", nencoding = \"UTF-8\")\n",
    "oracle_cursor = oracle_connection.cursor()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='white', rc={'figure.figsize':(12,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, msg='operation', verbose=True):\n",
    "        self.msg = msg\n",
    "        self.verbose = verbose       \n",
    "    def __enter__(self, ):\n",
    "        self.start = time.clock()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.clock()\n",
    "        self.interval = self.end - self.start\n",
    "        if self.verbose:\n",
    "            print('{} took {:.3f}s'.format(self.msg, self.interval), flush=True)\n",
    "\n",
    "\n",
    "def df_categorical_variables_stat(df, max_cardinality=200):\n",
    "    for colname in df.columns:\n",
    "        print('next_column: {}'.format(colname))\n",
    "        uniq_vals = df[colname].unique()\n",
    "        nunique = len(uniq_vals)\n",
    "        print(\"# of uniqs: {}\".format(nunique))\n",
    "        if nunique < max_cardinality:\n",
    "            print('Viable Categorical. Value counts:')\n",
    "            print(df[colname].value_counts(dropna=False))\n",
    "        else:\n",
    "            print('High cardinality/Non categorical')\n",
    "        print((('-' * 80) + '\\n')*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read susp_ops took 3.212s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4,9,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read susp_members took 3.570s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,21,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,8,9,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read off_ops took 41.536s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (2,3,5,11,17,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read off_ops took 36.941s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del susp_ops\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del susp_members\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    del off_ops\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del off_members\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with Timer('read susp_ops', True):\n",
    "    susp_ops = pd.read_csv('../../data/susp_ops.csv')\n",
    "with Timer('read susp_members', True):\n",
    "    susp_members = pd.read_csv('../../data/susp_members.csv')\n",
    "with Timer('read off_ops', True):\n",
    "    off_ops = pd.read_csv('../../data/off_ops.csv')\n",
    "with Timer('read off_ops', True):\n",
    "    off_members = pd.read_csv('../../data/off_members.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process off_ops took 4.840s\n"
     ]
    }
   ],
   "source": [
    "with Timer('process off_ops', True):\n",
    "    off_ops.loc[:, 'P_OPERATIONDATETIME'] = pd.to_datetime(off_ops['P_OPERATIONDATETIME'])\n",
    "    off_ops.sort_values(by='P_OPERATIONDATETIME', kind='mergesort', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "* process_client_indices() --- parse numeric indices with small typo errors to ints, and encode non-digit ids with negative numbers\n",
    "* flatten_by_column() --- each operation_id in OFF_OPERATIONS has 1-3 linked members with disjoint P_CLIENTROLE values. We want unique operation_id per row, with linked members with diferent roles spreading into a row columns.\n",
    "* join_ops_with_flatten_members() --- join operations dataframe with flattened operation id dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_client_indices(client_indices, trivial_to_nontrivial=None):\n",
    "    if trivial_to_nontrivial is None:\n",
    "        trivial_ids_to_nontrivial = defaultdict(lambda: -len(trivial_ids_to_nontrivial) - 2)\n",
    "\n",
    "    nontrivial_clients_ids = client_indices.copy()\n",
    "    nontrivial_clients_ids[nontrivial_clients_ids.isnull()] = -1\n",
    "\n",
    "    nontrivial_clients_ids = nontrivial_clients_ids.map(str)\n",
    "\n",
    "    nontrivial_ids = nontrivial_clients_ids.map(lambda s: s.strip())\n",
    "\n",
    "    is_trivial = nontrivial_ids.map(lambda s: len(re.findall('^([\\d]+|-[\\d]+)', s)) == 0)\n",
    "    trivial_ids = nontrivial_ids[is_trivial]\n",
    "\n",
    "    for ti in trivial_ids:\n",
    "        trivial_ids_to_nontrivial[ti]\n",
    "    trivial_ids_to_nontrivial\n",
    "    nontrivial_ids[is_trivial] = trivial_ids.map(lambda s: str(trivial_ids_to_nontrivial[s]))\n",
    "\n",
    "    nontrivial_ids = nontrivial_ids.map(lambda s: re.findall('^([\\d]+|-[\\d]+)', s)[0]).map(int)\n",
    "    return nontrivial_ids, trivial_to_nontrivial\n",
    "\n",
    "\n",
    "def flatten_by_column(df, to_flatten, flatten_by, group_by, names_flatten_by=None):\n",
    "    if names_flatten_by is None:\n",
    "        names_flatten_by = {\n",
    "            uniq_val: '{}={}'.format(flatten_by, uniq_val)\n",
    "            for uniq_val in df[flatten_by].unique()\n",
    "        }\n",
    "    #column_to_flatten_by = 'P_CLIENTROLE'\n",
    "    #column_to_group_by = 'P_SUSPICIOUSOPERATIONID'\n",
    "    #column_to_flatten = 'P_CLIENTID'\n",
    "    column_to_flatten_by = flatten_by\n",
    "    column_to_flatten = to_flatten\n",
    "    column_to_group_by = group_by\n",
    "    frames_to_join = [\n",
    "        df[[column_to_group_by, column_to_flatten]][df[column_to_flatten_by] == uniq_val]\n",
    "        for uniq_val in df[column_to_flatten_by].unique()\n",
    "    ]\n",
    "\n",
    "    for frame, uniq_val in zip(frames_to_join, df[flatten_by].unique()):\n",
    "        frame.rename(columns={column_to_flatten: names_flatten_by[uniq_val]}, inplace=True)\n",
    "        frame.set_index(column_to_group_by, inplace=True)\n",
    "\n",
    "    res = frames_to_join[0].join(frames_to_join[1:], how='outer')\n",
    "    return res\n",
    "\n",
    "\n",
    "def join_ops_with_flatten_members(ops, flatten_ops_with_members, id_colname='ID'):\n",
    "    ops_to_join = ops.set_index(id_colname, drop=False)\n",
    "    joined_ops = ops_to_join.join(flatten_ops_with_members, how='left')\n",
    "    joined_ops.reset_index(drop=True, inplace=True)\n",
    "    return joined_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse P_CLIENTID for offline members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "off_members client_ids processing took 48.448s\n"
     ]
    }
   ],
   "source": [
    "with Timer('off_members client_ids processing', True):\n",
    "    off_members.loc[:, 'P_CLIENTID'], trivial_ids_to_nontrivial = process_client_indices(off_members.P_CLIENTID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P_CLIENTROLE has only 3 values and not more than 3 P_CLIENTID with different P_CLIENTROLE corresponds to a single P_OPERATIONID value. So flatten them by P_CLIENTROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten grouped P_OPERATIONID with P_CLIENTID by P_CLIENTROLE took 19.120s\n",
      "join offline operations with flattened members info took 6.706s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del off_res\n",
    "except:\n",
    "    pass\n",
    "\n",
    "names_by_clientrole = {\n",
    "    1: 'id_from',\n",
    "    2: 'id_to',\n",
    "    3: 'id_susp_3',\n",
    "    4: 'id_susp_4',\n",
    "    5: 'id_beneficial',\n",
    "    6: 'id_susp_6'\n",
    "}\n",
    "\n",
    "with Timer('flatten grouped P_OPERATIONID with P_CLIENTID by P_CLIENTROLE', True):\n",
    "    off_res = flatten_by_column(off_members, 'P_CLIENTID', 'P_CLIENTROLE', 'P_OPERATIONID', names_by_clientrole)\n",
    "\n",
    "off_res = off_res.reset_index().drop_duplicates().set_index('P_OPERATIONID', drop=True)\n",
    "try:\n",
    "    del joined_offline_ops\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with Timer('join offline operations with flattened members info', True):\n",
    "    joined_offline_ops = join_ops_with_flatten_members(off_ops, off_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make 0-based indices for every operation and client we met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building operation_id counter took 10.409s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del operationid_counter\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del operationid_inv_counter\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with Timer('building operation_id counter', True):\n",
    "    if os.path.exists('operationid_counter.pickle'):\n",
    "        with open('operationid_counter.pickle', 'rb') as handle:\n",
    "            operationid_counter = defaultdict(lambda: len(operationid_counter), pickle.load(handle))\n",
    "    else:\n",
    "        operationid_counter = defaultdict(lambda: len(operationid_counter))\n",
    "        with open('operationid_counter.pickle', 'wb') as handle:\n",
    "            pickle.dump(dict(operationid_counter), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    joined_offline_ops.loc[:, 'ID'] = joined_offline_ops['ID'].map(operationid_counter.__getitem__)\n",
    "    operationid_inv_counter = {v: k for k, v in operationid_counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building client_id counter took 1.333s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del clientid_counter\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del clientid_inv_counter\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with Timer('building client_id counter', True):\n",
    "    if os.path.exists('clientid_counter.pickle'):\n",
    "        with open('clientid_counter.pickle', 'rb') as handle:\n",
    "            clientid_counter = defaultdict(lambda: len(clientid_counter), pickle.load(handle))\n",
    "    else:\n",
    "        clientid_counter = defaultdict(lambda: len(clientid_counter))\n",
    "        for row_num, row in tqdm(joined_offline_ops[['ID', 'id_from', 'id_to', 'id_beneficial']].iterrows()):\n",
    "            for client_id in [row.id_from, row.id_to, row.id_beneficial]:\n",
    "                if np.isfinite(client_id):\n",
    "                    clientid_counter[client_id]\n",
    "        with open('clientid_counter.pickle', 'wb') as handle:\n",
    "            pickle.dump(dict(clientid_counter), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    clientid_inv_counter = {v: k for k, v in clientid_counter.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set 'target' column with susp_ops P_SENDTOKFMBOOL values. Normal transactions will have value -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "susp_ops = susp_ops[~susp_ops.P_OFFLINEOPERATIONID.isnull()].copy()\n",
    "susp_ops.loc[susp_ops.P_OPERATION_LIST.isnull(), 'P_OPERATION_LIST'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_offline_ops.loc[:, 'P_OPERATIONDATETIME'] = pd.to_datetime(joined_offline_ops.P_OPERATIONDATETIME)\n",
    "joined_offline_ops.set_index('ID', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_offline_ops['raw_id_from'] = joined_offline_ops.id_from.map(clientid_counter.get)\n",
    "joined_offline_ops['raw_id_to'] = joined_offline_ops.id_to.map(clientid_counter.get)\n",
    "joined_offline_ops['raw_id_beneficial'] = joined_offline_ops.id_beneficial.map(clientid_counter.get)\n",
    "joined_offline_ops['seconds_from_start'] = (joined_offline_ops.P_OPERATIONDATETIME - \\\n",
    "                                            joined_offline_ops.P_OPERATIONDATETIME.min()).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401500it [00:22, 18117.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting proper labels for learning took 22.347s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer('setting proper labels for learning'):\n",
    "    target_mapped = defaultdict(lambda: -1)\n",
    "\n",
    "    for i, row in tqdm(susp_ops[['P_OFFLINEOPERATIONID', 'P_SENDTOKFMBOOL']].iterrows()):\n",
    "        off_op_id, to_kfm = operationid_counter[int(row.P_OFFLINEOPERATIONID)], row.P_SENDTOKFMBOOL\n",
    "        if off_op_id not in target_mapped:\n",
    "            target_mapped[off_op_id] = 0 if to_kfm == 2 else 1\n",
    "        else:\n",
    "            cur_target = target_mapped[off_op_id]\n",
    "            if cur_target == 0 and to_kfm != 2:\n",
    "                target_mapped[off_op_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_offline_ops['target'] = joined_offline_ops.ID.map(target_mapped.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_flow_columns = [\n",
    "    'ID',\n",
    "    'P_OPERATIONDATETIME',\n",
    "    'seconds_from_start',\n",
    "    'P_BASEAMOUNT',\n",
    "    'raw_id_from',\n",
    "    'raw_id_to',\n",
    "    'target'\n",
    "]\n",
    "\n",
    "selected_scoring_columns = [\n",
    "    'ID',\n",
    "    'P_ISSUEDBID',\n",
    "    'P_BASEAMOUNT',\n",
    "    'P_BRANCH',\n",
    "    'P_CURRENCYCODE',\n",
    "    'P_EKNPCODE',\n",
    "    'P_DOCCATEGORY',\n",
    "    'P_KFM_OPER_REASON',\n",
    "    'P_BS_OPER_TYPE',\n",
    "    'P_WAS_SEND',\n",
    "    'target'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_flow = joined_offline_ops[selected_flow_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192296"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans_flow.seconds_from_start.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8222263"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
